{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STWYYHKikmtE"
      },
      "outputs": [],
      "source": [
        "#¬†Before we begin, run this cell if you are using Colab\n",
        "!git clone -b 3-ysi-tutorial https://github.com/nestauk/im-tutorials.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S39DhcwZkmtG"
      },
      "source": [
        "# YSI Tutorial - Web Scraping\n",
        "\n",
        "## Structure\n",
        "1. HTML pages\n",
        "2. Chrome DevTools\n",
        "3. Web scraping packages\n",
        "    * BeautifulSoup\n",
        "        * Exercise\n",
        "    * Selenium\n",
        "        * Exercise\n",
        "4. Ethical considerations of web scraping\n",
        "\n",
        "## What you will be able to do after the tutorial\n",
        "* Inspect an HTML page and identify which parts you want to scrape.\n",
        "* Scrape web pages with `requests` and `BeautifulSoup`.\n",
        "* Navigate Javascript elements with `Selenium`\n",
        "* Judge when web scraping is the most suitable approach and what you should consider before doing so (be a good citizen of the Internet).\n",
        "\n",
        "## HTML page structure\n",
        "\n",
        "**Hypertext Markup Language (HTML)** is the standard markup language for documents designed to be displayed in a web browser. HTML describes the structure of a web page and it can be used with **Cascading Style Sheets (CSS)** and a scripting language such as **JavaScript** to create interactive websites. HTML consists of a series of elements that \"tell\" to the browser how to display the content. Lastly, elements are represented by **tags**.\n",
        "\n",
        "Here are some tags:\n",
        "* `<!DOCTYPE html>` declaration defines this document to be HTML5.  \n",
        "* `<html>` element is the root element of an HTML page.  \n",
        "* `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n",
        "* `<head>` element contains meta information about the document.  \n",
        "* `<title>` element specifies a title for the document.  \n",
        "* `<body>` element contains the visible page content.  \n",
        "* `<h1>` element defines a large heading.  \n",
        "* `<p>` element defines a paragraph.  \n",
        "* `<a>` element defines a hyperlink.\n",
        "\n",
        "HTML tags normally come in pairs like `<p>` and `</p>`. The first tag in a pair is the opening tag, the second tag is the closing tag. The end tag is written like the start tag, but with a slash inserted before the tag name.\n",
        "\n",
        "<img src=\"https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/tags.png?raw=1\" width=\"512\">\n",
        "\n",
        "HTML has a tree-like üå≥ üå≤ structure thanks to the **Document Object Model (DOM)**, a cross-platform and language-independent interface. Here's how a very simple HTML tree looks like.\n",
        "\n",
        "<img src=\"https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/dom_tree.gif?raw=1\">\n",
        "\n",
        "### Creating a simple HTML page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfE39FN8kmtI"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5kl8J8vkmtI"
      },
      "outputs": [],
      "source": [
        "display(HTML(\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\" dir=\"ltr\">\n",
        "<head>\n",
        "  <title>Intro to HTML</title>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "  <h1>Heading h1</h1>\n",
        "  <h2>Heading h2</h2>\n",
        "  <h3>Heading h3</h3>\n",
        "  <h4>Heading h4</h4>\n",
        "\n",
        "  <p>\n",
        "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
        "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
        "  </p>\n",
        "\n",
        "  <p>\n",
        "    This <br> is a paragraph <br> with <br> line breaks\n",
        "  </p>\n",
        "\n",
        "  <p style=\"color:red\">\n",
        "    Add colour to your paragraphs.\n",
        "  </p>\n",
        "\n",
        "  <p>Unordered list:</p>\n",
        "  <ul>\n",
        "    <li>Python</li>\n",
        "    <li>R</li>\n",
        "    <li>Julia</li>\n",
        "  </ul>\n",
        "\n",
        "  <p>Ordered list:</p>\n",
        "  <ol>\n",
        "    <li>Data collection</li>\n",
        "    <li>Exploratory data analysis</li>\n",
        "    <li>Data analysis</li>\n",
        "    <li>Policy recommendations</li>\n",
        "  </ol>\n",
        "  <hr>\n",
        "\n",
        "  <!-- This is a comment -->\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7f26yhkmtI"
      },
      "source": [
        "## Chrome DevTools\n",
        "\n",
        "[Chrome DevTools](https://developers.google.com/web/tools/chrome-devtools/) is a set of web developer tools built directly into the Google Chrome browser. DevTools can help you view and edit web pages. We will use Chrome's tool to inspect an HTML page and find which elements correspond to the data we might want to scrape.\n",
        "\n",
        "### Short exercise\n",
        "To get some experience with the HTML page structure and Chrome DevTools, we will search and locate elements in [IMDB](https://www.imdb.com/).\n",
        "\n",
        "**Tip**: Hit *Command+Option+C* (Mac) or *Control+Shift+C* (Windows, Linux) to access the elements panel.\n",
        "\n",
        "#### Tasks (we will do them together)\n",
        "* Find the _Sign in_ button\n",
        "* Find the IMDB logo.\n",
        "* Find the box containing the _Now Playing_ table.\n",
        "* Locate one of the photos/videos in the main section of the page.\n",
        "* What is the _heading_ size of the titles in the main section of the page?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IfCXY4jkmtJ"
      },
      "source": [
        "## Web Scraping with `requests` and `BeautifulSoup`\n",
        "\n",
        "We will use `requests` and `BeautifulSoup` to access and scrape the content of [IMDB's homepage](https://www.imdb.com).\n",
        "\n",
        "### What is `BeautifulSoup`?\n",
        "\n",
        "It is a Python library for pulling data out of HTML and XML files. It provides methods to navigate the document's tree structure that we discussed before and scrape its content.\n",
        "\n",
        "### Our pipeline\n",
        "<img src='https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/scrape-pipeline.png?raw=1' width=\"1024\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atGE7oufkmtJ"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2D24E05kmtJ"
      },
      "outputs": [],
      "source": [
        "#¬†IMDB's homepage\n",
        "imdb_url = 'https://www.imdb.com'\n",
        "\n",
        "#¬†Use requests to retrieve data from a given URL\n",
        "imdb_response = requests.get(imdb_url)\n",
        "\n",
        "#¬†Parse the whole HTML page using BeautifulSoup\n",
        "imdb_soup = BeautifulSoup(imdb_response.text, 'html.parser')\n",
        "\n",
        "#¬†Title of the parsed page\n",
        "imdb_soup.title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lwgHEVckmtJ"
      },
      "outputs": [],
      "source": [
        "#¬†We can also get it without the HTML tags\n",
        "imdb_soup.title.string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUQGfngekmtJ"
      },
      "source": [
        "### Collect trailers' title and description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "658x8zvYkmtK"
      },
      "outputs": [],
      "source": [
        "trailers = imdb_soup.find('div', {'class': 'ab_hero'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QpRe3__kmtK"
      },
      "outputs": [],
      "source": [
        "# print(trailers.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMHsUNdvkmtK"
      },
      "source": [
        "We will use the `.find_all()` method to search the HTML tree for particular tags and get a `list` with all the relevant objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaNU0PORkmtK"
      },
      "outputs": [],
      "source": [
        "for title, image in zip(trailers.find_all('div', {'class': 'onoverflow'}), trailers.find_all('img', {'class': 'pri_image'})):\n",
        "    print(f\"{title.text}: {image['title']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM2wczz6kmtK"
      },
      "source": [
        "###¬†Collect side bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BheAIXHkmtK"
      },
      "outputs": [],
      "source": [
        "for widget in imdb_soup.find_all('div', {'class': 'aux-content-widget-2'}):\n",
        "    #¬†Check that the widget has a heading\n",
        "    if widget.h3:\n",
        "        # Print the widget's heading along with the movie titles.\n",
        "        print(widget.h3.string)\n",
        "        for title in widget.find_all('div', {'class': 'title'}):\n",
        "            print(title.text)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYm9ihLKkmtK"
      },
      "source": [
        "### Collect articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf2qM0-AkmtK"
      },
      "outputs": [],
      "source": [
        "for article in imdb_soup.find_all('div', {'class': 'article'}):\n",
        "    if article.h3:\n",
        "        # Title of the article\n",
        "        print(article.h3.string)\n",
        "        # Text\n",
        "        print(article.p.text)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tSbq6c7kmtK"
      },
      "source": [
        "###¬†Find links\n",
        "\n",
        "In many cases, it is useful to collect the links contained in a webpage (for example, you might want to scrape them too). Here is how you can do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOhA3oSskmtK"
      },
      "outputs": [],
      "source": [
        "# Find all links\n",
        "links = [link.get('href') for link in imdb_soup.find_all('a')]\n",
        "\n",
        "#¬†Add homepage and keep the unique links\n",
        "fixed_links = set([''.join([imdb_url, link]) for link in links if link])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo2bHP7OkmtK"
      },
      "outputs": [],
      "source": [
        "# fixed_links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VglhM9SkmtK"
      },
      "source": [
        "##¬†From web data to analysis\n",
        "\n",
        "**[Box Office Mojo](https://www.boxofficemojo.com/)** is a website that tracks box office revenue in a systematic, algorithmic way. The site was founded in 1999 and was bought in 2008 by IMDb.  \n",
        "\n",
        "In this example, we will scrape data for UK's Weekend box office and create some simple plots. This will be our pipeline:\n",
        "\n",
        "<img src='https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/boxofficemojo-pipeline.png?raw=1' width='1024'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKYJGTa-kmtK"
      },
      "outputs": [],
      "source": [
        "#¬†Box Office Mojo - UK Weekend box office\n",
        "boxofficemojo_url = 'https://www.boxofficemojo.com/intl/uk/?yr=2019&wk=33&currency=local'\n",
        "\n",
        "#¬†Use requests to retrieve data from a given URL\n",
        "bom_response = requests.get(boxofficemojo_url)\n",
        "\n",
        "#¬†Parse the whole HTML page using BeautifulSoup\n",
        "bom_soup = BeautifulSoup(bom_response.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLW3HdjqkmtL"
      },
      "source": [
        "Let's inspect the webpage.\n",
        "\n",
        "Our goal is to scrape the main table. An [HTML Table](https://www.w3schools.com/html/html_tables.asp) is defined with the `<table>` tag. Each table row is defined with the `<tr>` tag. A table header is defined with the `<th>` tag. By default, table headings are bold and centered. A table data/cell is defined with the `<td>` tag and they can contain all sorts of HTML elements; text, images, lists, other tables, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYtkUKSekmtL"
      },
      "outputs": [],
      "source": [
        "# There are 7 tables in the Box Office Mojo page but we are interested in the one with the most data (table 5).\n",
        "print(f\"NUMBER OF TABLES IN THE PAGE: {len(bom_soup.find_all('table'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr2reWQjkmtL"
      },
      "outputs": [],
      "source": [
        "#¬†Python starts counting from 0\n",
        "table = bom_soup.find_all('table')[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTPW3kIWkmtL"
      },
      "outputs": [],
      "source": [
        "# table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EcUuLnakmtL"
      },
      "source": [
        "There are various ways to access the content of a **table row**. We could:\n",
        "\n",
        "* Use the `.contents` method to get a `list` with the data of every table cell. Note that the list will contain multiple _newline characters_.\n",
        "* Use the `.text` method to get a `string` with the data of every table cell. Note that the data will be separated by a _newline character_ which is omitted when using `print()`. You can use the `.split(\\n)` method to split the `string` on `\\n`\n",
        "* Use `.find_all('td')` to get a list of all the data in a row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "---X5rl8kmtL"
      },
      "outputs": [],
      "source": [
        "#¬†Using the .contents method\n",
        "table.find_all('tr')[2].contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSSkxOT5kmtL"
      },
      "outputs": [],
      "source": [
        "# Using .text method\n",
        "table.find_all('tr')[2].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVsTIGo6kmtL"
      },
      "outputs": [],
      "source": [
        "#¬†Print text \"consumes\" the newline characters\n",
        "print(table.find_all('tr')[2].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Filtz6RxkmtL"
      },
      "outputs": [],
      "source": [
        "#¬†Split string on newline characters\n",
        "table.find_all('tr')[2].text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34wK9TUkmtL"
      },
      "outputs": [],
      "source": [
        "# Loop through the cells of a row and print their data\n",
        "for data in table.find_all('tr')[2].find_all('td'):\n",
        "    print(data.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRrfLjVykmtL"
      },
      "outputs": [],
      "source": [
        "# Table's column names\n",
        "for data in table.find_all('tr')[1].find_all('td'):\n",
        "    print(data.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUJoAjVTkmtL"
      },
      "source": [
        "We will use the `.find_all('td')` method to get the data from every table row.\n",
        "\n",
        "**Note**: We will not collect the first and last rows in this particular example. The first row contains all of the table data while the last row contains the buttons to other pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IfqNbWGkmtL"
      },
      "outputs": [],
      "source": [
        "# Loop over the table rows, collect the data and store them in a list.\n",
        "lst = []\n",
        "for row in table.find_all('tr')[1:-1]:\n",
        "    s = pd.Series([data.text for data in row.find_all('td')])\n",
        "    lst.append(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFFMEpjFkmtL"
      },
      "outputs": [],
      "source": [
        "#¬†Concatenate the Pandas Series in a DataFrame\n",
        "data = pd.concat(lst, axis=1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKkigX8mkmtL"
      },
      "outputs": [],
      "source": [
        "#¬†The first line contains the header - let's fix that!\n",
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ftIdK8HkmtL"
      },
      "outputs": [],
      "source": [
        "# grab the first row for the header\n",
        "header = []\n",
        "for col in data.iloc[0, :-1]:\n",
        "    if '/' not in col:\n",
        "        header.append(col)\n",
        "    else:\n",
        "        header.extend(col.split('/'))\n",
        "\n",
        "data = data[1:] # take the data less the header row\n",
        "data.columns = header # set the header row as the df header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11pNkzfGkmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Replace the n/a string with a Null value.\n",
        "data.replace('n/a', np.nan, inplace=True)\n",
        "data.replace('-', np.nan, inplace=True)\n",
        "\n",
        "#¬†Remove the ¬£ symbol from the \"Gross\" column and transform strings to integers\n",
        "data['Weekend Gross'] = data['Weekend Gross'].apply(lambda x: int(x[1:].replace(',', '')))\n",
        "data['Gross-to-Date'] = data['Gross-to-Date'].apply(lambda x: int(x[1:].replace(',', '')))\n",
        "\n",
        "#¬†Transform strings to integers\n",
        "data['Theaters'] = data['Theaters'].apply(lambda x: int(x) if isinstance(x, str) else x)\n",
        "data['Week'] = data['Week'].apply(lambda x: int(x) if isinstance(x, str) else x)\n",
        "\n",
        "#¬†Create a new variable showing how much a movie grossed on average on weekly basis\n",
        "data['Week AVG'] = data['Gross-to-Date'].div(data['Week'])\n",
        "\n",
        "#¬†Set the movie title as index\n",
        "data.set_index('Movie', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW3qY2GJkmtR"
      },
      "outputs": [],
      "source": [
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Miq1qwHmkmtR"
      },
      "outputs": [],
      "source": [
        "print(f'(MOVIES, COLUMNS) -> {data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDj8PgJCkmtR"
      },
      "outputs": [],
      "source": [
        "print(f'% OF MISSING VALUES PER COLUMN\\n{(data.isnull().sum() / data.shape[0]) * 100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfDBBGzDkmtR"
      },
      "outputs": [],
      "source": [
        "# Use the .value_counts() method to count the number of studios\n",
        "data.Studio.value_counts().plot(kind='bar', title='Studios with the most movies in the top 55')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYqeyj_9kmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Use the .sort_values() method to sort the values of a column\n",
        "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,8))\n",
        "\n",
        "# ax1\n",
        "data['Week AVG'].sort_values(ascending=False)[:25].plot(kind='bar', title='Weekly Gross earnings', ax=ax1)\n",
        "# ax2\n",
        "data['Theaters'].sort_values(ascending=False)[:25].plot(kind='bar', title='Number of theaters showing a movie', ax=ax2)\n",
        "\n",
        "f.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSyvFsRDkmtR"
      },
      "source": [
        "##¬†Exercise\n",
        "\n",
        "Scrape the main table of [UK's Yearly Box Office](https://www.boxofficemojo.com/intl/uk/yearly/) and try to answer the following:\n",
        "1. Which producers have the most films in the top 100?\n",
        "2. Can you find the gross earnings by distributor?\n",
        "\n",
        "[Link to the solutions. Don't cheat!](https://colab.research.google.com/github/nestauk/im-tutorials/blob/3-ysi-tutorial/notebooks/Web-Scraping/solutions.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW-J2XYwkmtR"
      },
      "outputs": [],
      "source": [
        "# That's the URL you should use\n",
        "url = 'https://www.boxofficemojo.com/intl/uk/yearly/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8LKKeH3kmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Access the webpage content\n",
        "# write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXgxxXRokmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Parse the HTML page\n",
        "#¬†write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nH4MnvPkmtR"
      },
      "outputs": [],
      "source": [
        "# Choose the relevant table\n",
        "#¬†write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2AY8VdLkmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Parse and store the data of every table row\n",
        "#¬†write your code here (~4 lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeLwDcHKkmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Concatenate the data in a Pandas DataFrame and place the first row of the DataFrame as header. Use the .head() method to check your DataFrame.\n",
        "#¬†write your code here (~5 lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30r8-yc1kmtR"
      },
      "outputs": [],
      "source": [
        "#¬†Q1: Count the films per distributor and plot them using a bar chart\n",
        "# write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNXppVp3kmtR"
      },
      "outputs": [],
      "source": [
        "# Q2: Gross earnings by distributors\n",
        "\n",
        "#¬†Remove the ¬£ symbol from the \"Gross\" column and transform strings to integers\n",
        "# write your code here (~1 line)\n",
        "\n",
        "# Group the data by Distributor and add the Gross value of their movies\n",
        "# write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzB0LkJHkmtR"
      },
      "source": [
        "##¬†Advanced web scraping tools\n",
        "\n",
        "**[Scrapy](https://scrapy.org)** is a Python framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format.\n",
        "\n",
        "**[ARGUS](https://github.com/datawizard1337/ARGUS)** is an easy-to-use web mining tool that's built on Scrapy. It is able to crawl a broad range of different websites.\n",
        "\n",
        "**[Selenium](https://selenium-python.readthedocs.io/index.html)** is an umbrella project encapsulating a variety of tools and libraries enabling web browser automation. Selenium specifically provides infrastructure for the W3C WebDriver specification ‚Äî a platform and language-neutral coding interface compatible with all major web browsers. We can use it to imitate a user's behaviour and interact with Javascript elements (buttons, sliders etc.).\n",
        "\n",
        "For now, let's see how Selenium works.\n",
        "\n",
        "### How to install Selenium\n",
        "1. If you are using Anaconda: `conda install -c conda-forge selenium `\n",
        "2. Download the driver for your web browser for [here](https://selenium-python.readthedocs.io/installation.html#drivers). **Note:** Choose a driver that corresponds to your web browser's version. Unzip the file and move the executable to your working directory.\n",
        "\n",
        "#### Important note on Selinium and web drivers\n",
        "If you are running this notebook locally, follow the above steps and run the code directly below (change the path to where your web driver is located). If you are running this notebook on colab, skip the next cell and run the one below it.\n",
        "\n",
        "### Scraping data with Selenium\n",
        "We will use [UK's Yearly Box Office](https://www.boxofficemojo.com/intl/uk/yearly/) to scrape not only the top 100 but all the top movies of 2019. This will be our pipeline:\n",
        "\n",
        "<img src='https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/selenium-pipeline.png?raw=1' width='1024'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNe55hptkmtS"
      },
      "outputs": [],
      "source": [
        "# # RUN THIS CELL WHEN USING THE NOTEBOOK LOCALLY - YOU SHOULD INSTALL SELENIUM FIRST\n",
        "# import selenium.webdriver\n",
        "# # Path to the Chrome driver for my Mac -- yours will differ\n",
        "# mac_path = '../../chromedriver'\n",
        "# driver = selenium.webdriver.Chrome(executable_path=mac_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCqdqFy7kmtS"
      },
      "outputs": [],
      "source": [
        "# # RUN THIS CELL WHEN USING THE NOTEBOOK ON COLAB - NO PREVIOUS INSTALLATION OF SELENIUM IS NEEDED\n",
        "# # install chromium, its driver, and selenium\n",
        "# !apt update\n",
        "# !apt install chromium-chromedriver\n",
        "# !pip install selenium\n",
        "# # set options to be headless\n",
        "# from selenium import webdriver\n",
        "# options = webdriver.ChromeOptions()\n",
        "# options.add_argument('--headless')\n",
        "# options.add_argument('--no-sandbox')\n",
        "# options.add_argument('--disable-dev-shm-usage')\n",
        "# # open it, go to a website, and get results\n",
        "# driver = webdriver.Chrome('chromedriver',options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smOOEJd0kmtS"
      },
      "outputs": [],
      "source": [
        "def html2df(source, q):\n",
        "    \"\"\"A wrapper of the scraping pipeline we used before.\"\"\"\n",
        "    #¬†Parse the HTML page\n",
        "    soup = BeautifulSoup(source, 'html.parser')\n",
        "\n",
        "    # Choose the relevant table\n",
        "    table = soup.find_all('table')[4]\n",
        "\n",
        "    #¬†Parse and store the data of every table row\n",
        "    lst = []\n",
        "    for row in table.find_all('tr'):\n",
        "        s = pd.Series([data.text for data in row.find_all('td')])\n",
        "        lst.append(s)\n",
        "\n",
        "    #¬†Concatenate the data in a Pandas DataFrame and place the first row of the DataFrame as header.\n",
        "    data = pd.concat(lst, axis=1).T\n",
        "\n",
        "    # Grab the first row for the header\n",
        "    new_header = data.iloc[0]\n",
        "\n",
        "    # Take the data less the header row\n",
        "    data = data[1:]\n",
        "\n",
        "    # Set the header row as the df header\n",
        "    data.columns = new_header\n",
        "\n",
        "    # Add a new column tagging the page we scraped\n",
        "    data['page'] = q\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47sUeuhKkmtS"
      },
      "source": [
        "### About the `driver`\n",
        "The `driver` object establishes a connection from this Python environment out to the browser window.\n",
        "\n",
        "**Note:** A new Chrome window will pop-up when you run the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fwXGg5lkmtS"
      },
      "outputs": [],
      "source": [
        "# URL to use in Selenium\n",
        "driver.get('https://www.boxofficemojo.com/intl/uk/yearly/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXOU5uYSkmtS"
      },
      "outputs": [],
      "source": [
        "lst = []\n",
        "lst.append(html2df(driver.page_source, '#1'))\n",
        "for i in ['#101', '#201', '#301', '#401']:\n",
        "    #¬†Locate Hyperlinks by partial link text\n",
        "    elem = driver.find_element_by_partial_link_text(i)\n",
        "    #¬†Click on the next page\n",
        "    elem.click()\n",
        "    # Store the Pandas DataFrame with the scraped content in a list\n",
        "    lst.append(html2df(driver.page_source, i))\n",
        "\n",
        "#¬†Concatenate all Pandas DataFrames\n",
        "df = pd.concat(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd1-BREmkmtS"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_42lexgkmtS"
      },
      "outputs": [],
      "source": [
        "print(f'(MOVIES, COLUMNS) -> {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1WbmzYNkmtS"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Use Selenium to scrape Box Office Mojo's top \\#100 for every year between 2002 and 2019.\n",
        "\n",
        "[Link to the solutions. Don't cheat!](https://colab.research.google.com/github/nestauk/im-tutorials/blob/3-ysi-tutorial/notebooks/Web-Scraping/solutions.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgmuao_ukmtS"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.boxofficemojo.com/intl/uk/yearly/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOUqv7tkmtS"
      },
      "outputs": [],
      "source": [
        "# URL to use in Selenium\n",
        "# write your code here (~1 line)\n",
        "\n",
        "lst = []\n",
        "# Loop over the years.\n",
        "# write your code here (~1-2 lines)\n",
        "\n",
        "    #¬†Locate Hyperlinks by partial link text\n",
        "    # write your code here (~1 line)\n",
        "\n",
        "    #¬†Click on the next page\n",
        "    # write your code here (~1 line)\n",
        "\n",
        "    # Store the Pandas DataFrame with the scraped content in a list\n",
        "    # write your code here (~1 line)\n",
        "\n",
        "#¬†Concatenate all Pandas DataFrames\n",
        "# write your code here (~1 line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4_t34dykmtS"
      },
      "source": [
        "##¬†Ethical considerations\n",
        "\n",
        "**You can scrape it, should you though?**\n",
        "\n",
        "A very good summary of practices for [ethical web scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
        "\n",
        "* If you have a public API that provides the data I‚Äôm looking for, I‚Äôll use it and avoid scraping all together.\n",
        "* I will only save the data I absolutely need from your page.\n",
        "* I will respect any content I do keep. I‚Äôll never pass it off as my own.\n",
        "* I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
        "* I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
        "* I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
        "\n",
        "Some other [important components](http://robertorocha.info/on-the-ethics-of-web-scraping/) of ethical web scraping practices include:\n",
        "\n",
        "* Read the Terms of Service and Privacy Policies of a website before scraping it (this might not be possible in many situations though).\n",
        "* If it‚Äôs not clear from looking at the website, contact the webmaster and ask if and what you‚Äôre allowed to harvest.\n",
        "* Be gentle on smaller websites\n",
        "    * Run your scraper in off-peak hours\n",
        "    * Space out your requests.\n",
        "* Identify yourself by name and email in your User-Agent strings.\n",
        "* Inspecting the **robots.txt** file for rules about what pages can be scraped, indexed, etc.\n",
        "\n",
        "### What is a robots.txt?\n",
        "\n",
        "A simple text file placed on the web server which tells crawlers which file they can and cannot access. It's also called _The Robots Exclusion Protocol_.\n",
        "\n",
        "<img src='https://github.com/nestauk/im-tutorials/blob/3-ysi-tutorial/figures/Web-Scraping/robots.png?raw=1' width=\"600\">\n",
        "\n",
        "####¬†Some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNOC9QHZkmtS"
      },
      "outputs": [],
      "source": [
        "print(requests.get('https://www.nesta.org.uk/robots.txt').text)\n",
        "print('-----')\n",
        "print(requests.get('https://www.boxofficemojo.com/robots.txt').text)\n",
        "print('-----')\n",
        "print(requests.get('https://www.howtogeek.com/robots.txt').text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FcdEyyHkmtS"
      },
      "source": [
        "#### What's a User-Agent?\n",
        "\n",
        "A User-Agent is a string identifying the browser and operating system to the web server. It's your machine's way of saying _Hi, I am Chrome on macOS_ to a web server.\n",
        "\n",
        "Web servers use user agents for a variety of purposes:\n",
        "* Serving different web pages to different web browsers. This can be used for good ‚Äì for example, to serve simpler web pages to older browsers ‚Äì or evil ‚Äì for example, to display a ‚ÄúThis web page must be viewed in Internet Explorer‚Äù message.\n",
        "* Displaying different content to different operating systems ‚Äì for example, by displaying a slimmed-down page on mobile devices.\n",
        "* Gathering statistics showing the browsers and operating systems in use by their users. If you ever see browser market-share statistics, this is how they‚Äôre acquired.\n",
        "\n",
        "Let's break down the structure of a human-operated User-Agent:\n",
        "\n",
        "```Mozilla/5.0 (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Mobile/7B405```\n",
        "\n",
        "The components of this string are as follows:\n",
        "\n",
        "* Mozilla/5.0: Previously used to indicate compatibility with the Mozilla rendering engine.\n",
        "* (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us): Details of the system in which the browser is running.\n",
        "* AppleWebKit/531.21.10: The platform the browser uses.\n",
        "* (KHTML, like Gecko): Browser platform details.\n",
        "* Mobile/7B405: This is used by the browser to indicate specific enhancements that are available directly in the browser or through third parties. An example of this is Microsoft Live Meeting which registers an extension so that the Live Meeting service knows if the software is already installed, which means it can provide a streamlined experience to joining meetings.\n",
        "\n",
        "When scraping websites, it is a good idea to include your contact information as a custom **User-Agent** string so that the webmaster can get in contact. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU1rm-NpkmtS"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Kostas Stathoulopoulos bot',\n",
        "    'From': 'konstantinos.stathoulopoulos@nesta.org.uk'\n",
        "}\n",
        "request = requests.get('https://www.nesta.org.uk/', headers=headers)\n",
        "print(request.request.headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZeSimXvkmtT"
      },
      "source": [
        "##¬†Additional resources/references:\n",
        "\n",
        "* [Document Object Model (DOM)](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction)\n",
        "* [HTML elements reference guide](https://www.w3schools.com/tags/default.asp)\n",
        "* [About /robots.txt](https://www.robotstxt.org/robotstxt.html)\n",
        "* [The robots.txt file](https://varvy.com/robottxt.html)\n",
        "* [Ethics in Web Scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n",
        "* [On the Ethics of Web Scraping](http://robertorocha.info/on-the-ethics-of-web-scraping/)\n",
        "* [User-Agent](https://en.wikipedia.org/wiki/User_agent)\n",
        "* [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "* [Selinium Python - Unofficial documentation](https://selenium-python.readthedocs.io/)\n",
        "* [ARGUS paper](http://ftp.zew.de/pub/zew-docs/dp/dp18033.pdf)\n",
        "* [Brian's C. Keegan](http://www.brianckeegan.com/) excellent [5-week web scraping course](https://github.com/CU-ITSS/Web-Data-Scraping-S2019) intended for researchers in the social sciences and humanities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-OKFLGokmtT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py36]",
      "language": "python",
      "name": "conda-env-py36-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}