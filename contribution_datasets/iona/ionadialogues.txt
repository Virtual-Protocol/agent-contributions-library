import requests
from bs4 import BeautifulSoup
import time
import random

# List of URLs to scrape
urls = [
    'https://example.com/iona-article-1',
    'https://example.com/iona-article-2',
    # Add more URLs as needed
]

# Headers to avoid detection
headers_list = [
    {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    },
    {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36"
    },
    # Add more user agents if needed
]

# Function to scrape a single URL
def scrape_url(url):
    try:
        response = requests.get(url, headers=random.choice(headers_list))
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            paragraphs = soup.find_all('p')
            text_content = [para.get_text() for para in paragraphs]
            return text_content
        else:
            print(f"Failed to retrieve content from {url}")
            return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

# Scrape all URLs
all_texts = []
for url in urls:
    all_texts.extend(scrape_url(url))
    time.sleep(random.uniform(1, 3))  # Adding delay to avoid detection

# Save the scraped data to a text file
with open('iona_quotes.txt', 'w', encoding='utf-8') as file:
    for line in all_texts:
        file.write(line + "\n")

print("Scraping completed and data saved to iona_quotes.txt")
